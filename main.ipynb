{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "529fa4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken, torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.TextDataset import TextDataset\n",
    "from utils.GPTLargeLanguageModel import GPTLargeLanguageModel\n",
    "from utils.generate_tokens import generate_tokens\n",
    "\n",
    "def get_text(file):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def text_to_tokens(text, tokenizer):\n",
    "    text = tokenizer.encode(text)\n",
    "    text = torch.tensor(text).unsqueeze(0)\n",
    "    return text\n",
    "\n",
    "def tokens_to_text(tokens, tokenizer):\n",
    "    tokens = tokens.squeeze(0).tolist()\n",
    "    tokens = tokenizer.decode(tokens)\n",
    "    return tokens\n",
    "\n",
    "TRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "557874e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text data\n",
    "text = get_text(\"The_Complete_Works_of_William_Shakespeare.txt\")\n",
    "\n",
    "# LLM parameters\n",
    "vocab_size = 50257 # 50257\n",
    "num_layers = 12 # 12\n",
    "context_length = 1024 # 1024\n",
    "dimension = 768 # 768\n",
    "num_heads = 12 # 12\n",
    "dropout = 0.1 # 0.1\n",
    "\n",
    "# Dataloader parameters\n",
    "batch_size = 4\n",
    "stride = context_length // 2\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Dataloader\n",
    "dataset = TextDataset(text, tokenizer, max_length=context_length, stride=stride)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=0)\n",
    "\n",
    "# LLM\n",
    "LLM = GPTLargeLanguageModel(vocab_size, num_layers, context_length, dimension, num_heads, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1db151c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, epochs, device=device):\n",
    "    try:\n",
    "        loader_count = len(dataloader)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            print(f\"\\nEpoch {epoch}\")\n",
    "            model.train()\n",
    "\n",
    "            for i, (x, y) in enumerate(dataloader):\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(x)\n",
    "                loss = nn.functional.cross_entropy(output.flatten(0, 1), y.flatten())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                del output\n",
    "\n",
    "                print(f\"Loss {i+1}/{loader_count}: {loss:3f}\", end=\"\\r\")\n",
    "    except KeyboardInterrupt:\n",
    "        torch.save(LLM.state_dict(), f'interrupted_model_weights_{num_layers}_{context_length}_{num_heads}.pth')\n",
    "        torch.save(optimizer.state_dict(), f'interrupted_optimizer_{num_layers}_{context_length}_{num_heads}.pth')\n",
    "    finally: \n",
    "        print(\"Done\")\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(LLM.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "if TRAIN:\n",
    "    train_model(LLM, dataloader, optimizer, epochs=1)\n",
    "    torch.save(LLM.state_dict(), f'model_weights_{num_layers}_{context_length}_{num_heads}.pth')\n",
    "    torch.save(optimizer.state_dict(), f'optimizer_{num_layers}_{context_length}_{num_heads}.pth')\n",
    "else:\n",
    "    LLM.load_state_dict(torch.load(f'model_weights_{num_layers}_{context_length}_{num_heads}.pth'))\n",
    "    # optimizer.load_state_dict(torch.load(f'optimizer_{num_layers}_{context_length}_{num_heads}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6737ae62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm tired to see of all in such a man as goodly a king of a\n",
      "\n",
      "and a word.\n",
      "inbr of men must\n",
      "\n",
      "in a good of his\n",
      "I will to a goodly_.\n",
      "you were,\n",
      "\n",
      "with\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I'm tired\"\n",
    "output = generate_tokens(LLM, text_to_tokens(prompt, tokenizer).to(device), 50, 1024, 1.5, 25)\n",
    "print(tokens_to_text(output, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
